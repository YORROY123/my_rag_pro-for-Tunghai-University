import os
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
import chromadb
import google.generativeai as genai

# 1. è®€å– .env ä¸­çš„ Gemini API Key
load_dotenv()

class MyRAG:
    def __init__(self):
        print("ğŸš€ æ­£åœ¨å•Ÿå‹• RAG ç³»çµ±...")
        # è¼‰å…¥ Embedding æ¨¡å‹
        self.embed_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # åˆå§‹åŒ–å‘é‡è³‡æ–™åº«
        self.db_client = chromadb.PersistentClient(path="./chroma_db")
        self.collection = self.db_client.get_or_create_collection(name="knowledge_base")
        
        # 2. è¨­å®š Gemini (ä¿®æ­£é‡é»ï¼šç¢ºä¿ API KEY æœ‰è®€åˆ°)
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("âŒ æ‰¾ä¸åˆ° API Keyï¼Œè«‹æª¢æŸ¥ .env æª”æ¡ˆï¼")
            
        genai.configure(api_key=api_key)
        
        # ã€é—œéµä¿®æ­£ã€‘ï¼šç§»é™¤ models/ å‰ç¶´ï¼Œç›´æ¥ç”¨åç¨±
        # ä¿®æ”¹å‰ï¼šself.model = genai.GenerativeModel('gemini-1.5-flash')
        # ä¿®æ­£å¾Œï¼šå°æ‡‰ä½ æˆªåœ–ä¸­çš„ Gemini 2.5 Flash
        self.model = genai.GenerativeModel('gemini-2.5-flash')
        print("âœ… ç³»çµ±æº–å‚™å°±ç·’ï¼")

    def add_document(self, file_path):
        """è®€å–æª”æ¡ˆä¸¦å­˜å…¥è³‡æ–™åº«"""
        if not os.path.exists(file_path):
            print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{file_path}")
            return

        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # å„ªåŒ–åˆ†æ®µï¼šç”¨ \n\n (æ®µè½) åˆ†æ®µé€šå¸¸æ¯”å–®è¡Œæ›´å¥½
        chunks = [c.strip() for c in content.split('\n\n') if len(c.strip()) > 5]
        
        if not chunks:
            print("âš ï¸ æª”æ¡ˆå…§å®¹å¤ªå°‘ï¼Œæ²’æœ‰å¯å­˜å…¥çš„ç‰‡æ®µã€‚")
            return

        # è½‰æ›æˆå‘é‡
        embeddings = self.embed_model.encode(chunks).tolist()
        ids = [f"id_{i}_{os.path.basename(file_path)}" for i in range(len(chunks))]
        
        self.collection.add(documents=chunks, embeddings=embeddings, ids=ids)
        print(f"âœ… å·²æˆåŠŸå­˜å…¥ {len(chunks)} æ¢çŸ¥è­˜ç‰‡æ®µï¼")

    def ask(self, question):
        """æ ¸å¿ƒ RAG æµç¨‹"""
        # A. æª¢ç´¢ (Retrieval)
        query_vec = self.embed_model.encode([question]).tolist()
        results = self.collection.query(query_embeddings=query_vec, n_results=3)
        
        if not results['documents'][0]:
            return "è³‡æ–™åº«è£¡ç©ºç©ºçš„ï¼Œè«‹å…ˆ add_document é¤µæˆ‘åƒè³‡æ–™ã€‚"

        context = "\n---\n".join(results['documents'][0])
        
        # B. æç¤ºè©å·¥ç¨‹ (Prompt Engineering)
        prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ AI åŠ©æ‰‹ã€‚è«‹ã€åƒ…æ ¹æ“šã€ä¸‹æ–¹æä¾›çš„ã€åƒè€ƒè³‡æ–™ã€‘ä¾†å›ç­”å•é¡Œã€‚
è‹¥è³‡æ–™ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹å›ç­”ã€è³‡æ–™åº«ä¸­ç›®å‰æ²’æœ‰ç›¸é—œè³‡è¨Šã€ã€‚

ã€åƒè€ƒè³‡æ–™ã€‘ï¼š
{context}

ã€ä½¿ç”¨è€…æå•ã€‘ï¼š
{question}
"""
        # C. ç”Ÿæˆ (Generation)
        try:
            # ğŸ’¡ å¯«å¾—æ›´å¥½ï¼šä½¿ç”¨æœ€ç²¾ç°¡çš„åç¨±ï¼Œä¸¦åŠ ä¸ŠéŒ¯èª¤è™•ç†
            # æœ‰äº›ç’°å¢ƒéœ€è¦ 'gemini-1.5-flash'ï¼Œæœ‰äº›éœ€è¦ 'models/gemini-1.5-flash'
            # æˆ‘å€‘å˜—è©¦ç›´æ¥ç”¨æ¨¡å‹ç‰©ä»¶ä¾†ç”Ÿæˆ
            
            response = self.model.generate_content(prompt)
            return response.text
        except Exception as e:
            # å¦‚æœå¤±æ•—ï¼Œå˜—è©¦åˆ‡æ›æ¨¡å‹åç¨±æ ¼å¼
            try:
                print("ğŸ”„ æ­£åœ¨å˜—è©¦å‚™ç”¨æ¨¡å‹è·¯å¾‘...")
                temp_model = genai.GenerativeModel('models/gemini-1.5-flash')
                response = temp_model.generate_content(prompt)
                return response.text
            except:
                return f"âŒ å‘¼å« Gemini æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"